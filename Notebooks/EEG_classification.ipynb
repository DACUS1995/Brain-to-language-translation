{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.signal import convolve\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\tRECORDING_PATH_ROOT = \"D:\\Storage\\EEGRecordings\\Part_three\\\\\"\n",
    "\tRECORDING_NUM_SAMPLES = 700\n",
    "\tSENSORS_LABELS = [\"F3\", \"FC5\", \"AF3\", \"F7\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"F8\", \"AF4\", \"FC6\", \"F4\"]\n",
    "\tDATASET_TRAINING_VALIDATION_RATIO = 0.1\n",
    "\tCHECKPOINTS_DIR = './checkpoints'\n",
    "\tTENSORBOARD_LOGDIR = \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    if type(data) != np.ndarray:\n",
    "        raise Exception(\"Input data must be of type np.ndarray.\")\n",
    "    max_data = np.max(data)\n",
    "    min_data = np.min(data)\n",
    "    data = (data - min_data) / (max_data - min_data  + 1e-6)\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_single_signal(signal):\n",
    "\tsignal = np.copy(signal)\n",
    "\tif type(signal) != np.ndarray:\n",
    "\t\traise Exception(\"Input data must be of type np.ndarray.\")\n",
    "\tplt.figure()\n",
    "\tplt.plot(signal)\n",
    "\tplt.xticks(np.arange(signal.shape[0]))\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def plot_recording(data: np.ndarray):\n",
    "\tdata = np.copy(data)\n",
    "\tif type(data) != np.ndarray:\n",
    "\t\traise Exception(\"Input data must be of type np.ndarray.\")\n",
    "\t\n",
    "\tfor row_idx in range(data.shape[1]):\n",
    "\t\tdata[:, row_idx] = data[:, row_idx] + row_idx\n",
    "\n",
    "\tplt.figure()\n",
    "\tfor idx in range(data.shape[1]):\n",
    "\t\tplt.plot(data[:, idx])\n",
    "\tplt.xticks(np.arange(data.shape[0]))\n",
    "\tplt.show()\n",
    "\n",
    "def plot_mfcc(mfccs):\n",
    "\tplt.figure(figsize=(10, 4))\n",
    "\tdisplay.specshow(mfccs, x_axis='time')\n",
    "\tplt.colorbar()\n",
    "\tplt.title('MFCC')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "def show_gabor(I, **kwargs):\n",
    "\t# utility function to show image\n",
    "\tplt.figure()\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(I, cmap=plt.gray(), **kwargs)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recording_files_paths(mode=\"training\") -> List[List]:\n",
    "\t# data_root = os.path.join(Config.RECORDING_PATH_ROOT, \"\\Park\\Surdoiu_Tudor\\Day_1\")\n",
    "\tif mode == \"training\":\n",
    "\t\tdata_root = Config.RECORDING_PATH_ROOT + \"\\\\train\"\n",
    "\tif mode == \"testing\":\n",
    "\t\tdata_root = Config.RECORDING_PATH_ROOT + \"\\\\test\"\n",
    "\n",
    "\tdata_root = pathlib.Path(data_root)\n",
    "\tclasses_dir = list(data_root.glob('*'))\n",
    "\tprint(f\"Number of classes directories: [{len(classes_dir)}]\")\n",
    "\n",
    "\tall_file_paths = []\n",
    "\tnumber_of_samples = 0\n",
    "\n",
    "\tfor class_dir in classes_dir:\n",
    "\t\trecording_files = list(pathlib.Path(class_dir).glob('*'))\n",
    "\t\tall_file_paths.append([str(path) for path in recording_files])\n",
    "\t\tnumber_of_samples += len(recording_files)\n",
    "\n",
    "\tprint(f\"Scanned [{number_of_samples}] images\")\n",
    "\treturn all_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfcc(recording):\n",
    "\ttransformed_recording = None\n",
    "\n",
    "\tfor i in range(recording.shape[1]):\n",
    "\t\tcurrent_channel = librosa.feature.mfcc(recording[:, i], sr=160, n_mfcc=10, hop_length=10, n_fft=40)\n",
    "\t\tif transformed_recording is None:\n",
    "\t\t\ttransformed_recording = current_channel\n",
    "\t\telse:\n",
    "\t\t\ttransformed_recording = np.append(transformed_recording, current_channel, axis=0)\n",
    "\n",
    "\tassert transformed_recording is not None\n",
    "\t# print(transformed_recording.shape)\n",
    "\treturn transformed_recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recording(path, use_mfcc=False):\n",
    "\tdf = pd.read_csv(path, skiprows=[0], header=None, names=[\"COUNTER\", \"INTERPOLATED\", \"F3\", \"FC5\", \"AF3\", \"F7\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"F8\", \"AF4\", \"FC6\", \"F4\", \"RAW_CQ\", \"GYROX\"]) # \"GYROY\", \"MARKER\", \"MARKER_HARDWARE\", \"SYNC\", \"TIME_STAMP_s\", \"TIME_STAMP_ms\", \"CQ_AF3\", \"CQ_F7\", \"CQ_F3\", \"CQ_FC5\", \"CQ_T7\", \"CQ_P7\", \"CQ_O1\", \"CQ_O2\", \"CQ_P8\", \"CQ_T8\", \"CQ_FC6\", \"CQ_F4\", \"CQ_F8\", \"CQ_AF4\", \"CQ_CMS\", \"CQ_DRL\"])\n",
    "\n",
    "\tdf = df[:Config.RECORDING_NUM_SAMPLES]\n",
    "\tdf = df[Config.SENSORS_LABELS]\n",
    "\trecording = df.values\n",
    "\trecording.dtype = np.float64\n",
    "\trecording = normalization(recording)\n",
    "\n",
    "\tif recording.shape[0] < Config.RECORDING_NUM_SAMPLES:\n",
    "\t\trecording = np.pad(recording, ((0, Config.RECORDING_NUM_SAMPLES - recording.shape[0]), (0, 0)), mode=\"edge\")\n",
    "\t\n",
    "\tif recording.shape[0] != Config.RECORDING_NUM_SAMPLES:\n",
    "\t\traise Exception(f\"Session number of samples is super not OK: [{recording.shape[0]}]\")\n",
    "\n",
    "\tif use_mfcc:\n",
    "\t\trecording = compute_mfcc(recording)\n",
    "\n",
    "\trecording = np.transpose(recording)\n",
    "\trecording = recording.flatten()\n",
    "\treturn recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataset(shuffle=True, use_mfcc=False, use_gabor=False):\n",
    "\trecordings = []\n",
    "\tlabels = []\n",
    "\t# dateset_file_paths = tf.data.Dataset.from_tensor_slices(get_recording_files_paths())\n",
    "\n",
    "\tgabor_filters = [genGabor((40, 1), omega=i) for i in np.arange(0.1, 1, 0.3)]\n",
    "\tprint(f\"Number of gabor filters used: [{len(gabor_filters)}]\")\n",
    "\n",
    "\tfor n, class_file_list in enumerate(get_recording_files_paths()):\n",
    "\t\tfor m, file_path in enumerate(class_file_list):\n",
    "\t\t\trecording = load_recording(file_path, use_mfcc)\n",
    "\n",
    "\t\t\tif use_gabor == True:\n",
    "\t\t\t\trecording = np.empty((0), dtype=recording.dtype)\n",
    "\t\t\t\tfor gabor in gabor_filters:\n",
    "\t\t\t\t\trecording = np.append(recording, convolve(recording, gabor, mode=\"valid\"))\n",
    "\t\t\t\n",
    "\t\t\trecordings.append(recording)\n",
    "\t\t\tlabels.append(n)\n",
    "\n",
    "\treturn (recordings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_dataset(use_mfcc=False, use_gabor=False):\n",
    "\trecordings = []\n",
    "\tlabels = []\n",
    "\n",
    "\tgabor_filters = [genGabor((40, 1), omega=i) for i in np.arange(0.1, 1, 0.2)]\n",
    "\n",
    "\tfor n, class_file_list in enumerate(get_recording_files_paths(mode=\"testing\")):\n",
    "\t\tfor m, file_path in enumerate(class_file_list):\n",
    "\t\t\trecording = load_recording(file_path, use_mfcc)\n",
    "\n",
    "\t\t\tif use_gabor == True:\n",
    "\t\t\t\trecording = np.empty((0), dtype=recording.dtype)\n",
    "\t\t\t\tfor gabor in gabor_filters:\n",
    "\t\t\t\t\trecording = np.append(recording, convolve(recording, gabor, mode=\"valid\"))\n",
    "\t\t\t\n",
    "\t\t\trecordings.append(recording)\n",
    "\t\t\tlabels.append(n)\n",
    "\n",
    "\treturn (recordings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genGabor(sz, omega=0.5, theta=0, func=np.cos, K=np.pi):\n",
    "\tradius = (int(sz[0]/2.0), int(sz[1]/2.0))\n",
    "\t[x, y] = np.meshgrid(range(-radius[0], radius[0]+1), range(-radius[1], radius[1]+1))\n",
    "\n",
    "\tx1 = x * np.cos(theta) + y * np.sin(theta)\n",
    "\ty1 = -x * np.sin(theta) + y * np.cos(theta)\n",
    "    \n",
    "\tgauss = omega**2 / (4*np.pi * K**2) * np.exp(- omega**2 / (8*K**2) * ( 4 * x1**2 + y1**2))\n",
    "\tsinusoid = func(omega * x1) * np.exp(K**2 / 2)\n",
    "\tgabor = gauss * sinusoid\n",
    "\tgabor = np.array(gabor)\n",
    "\treturn gabor.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePipeline(pipeline, pipelineName, param_grid, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "\tsearch = GridSearchCV(pipeline, param_grid, iid=False, cv=5)\n",
    "\n",
    "\tsearch.fit(x_train, y_train)\n",
    "\tprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "\tprint(search.best_params_)\n",
    "\n",
    "\tresult = search.predict(x_test)\n",
    "\tresult_train = search.predict(x_train)\n",
    "\tprint(result)\n",
    "\n",
    "\tprint(pipelineName)\n",
    "\tprint(\"Accuracy:    \", accuracy_score(y_test, result))\n",
    "\tprint(\"Precission:  \", precision_score(y_test, result, average=\"binary\"))\n",
    "\tprint(\"Accuracy train:    \", accuracy_score(y_train, result_train))\n",
    "\tprint(\"Precission train:  \", precision_score(y_train, result_train, average=\"binary\"))\n",
    "\tprint(\"Recall:  \", recall_score(y_test, result, average=\"binary\"))\n",
    "\tprint(\"F1:  \", f1_score(y_test, result, average=\"binary\"))\n",
    "\tprint(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_pipeline(classifier):\n",
    "\treturn Pipeline([\n",
    "\t\t('pca', PCA(copy=True, iterated_power=10, n_components=None,\n",
    "                     random_state=None, svd_solver='randomized', tol=0.0,\n",
    "                     whiten=False)),\n",
    "        ('standardscaler', MinMaxScaler(copy=True)),\n",
    "\t\t(\"Classifier\", OneVsRestClassifier(classifier))\n",
    "\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gabor filters used: [3]\n",
      "Number of classes directories: [2]\n",
      "Scanned [318] images\n",
      "Number of classes directories: [2]\n",
      "Scanned [78] images\n",
      "(318, 9800)\n",
      "(318,)\n",
      "(78, 9800)\n",
      "(78,)\n"
     ]
    }
   ],
   "source": [
    "use_mfcc = False\n",
    "use_gabor = False\n",
    "\n",
    "x_train, y_train = create_training_dataset(use_mfcc=use_mfcc, use_gabor=False)\n",
    "x_test, y_test = create_testing_dataset(use_mfcc=use_mfcc, use_gabor=False)\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinge: (soft-margin) Support Vector Machines.\n",
    "Log: Logistic Regression.\n",
    "Least-Squares: Ridge Regression.\n",
    "Epsilon-Insensitive: (soft-margin) Support Vector Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.534):\n",
      "{'Classifier__estimator__alpha': 0.0001, 'Classifier__estimator__loss': 'squared_loss', 'Classifier__estimator__penalty': 'l1'}\n",
      "[0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1\n",
      " 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1\n",
      " 0 0 0 0]\n",
      "SGDClassifier\n",
      "Accuracy:     0.46153846153846156\n",
      "Precission:   0.45454545454545453\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-75e12d83e5c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_classification_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mevaluatePipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SGDClassifier\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-96cbfc0c5e1b>\u001b[0m in \u001b[0;36mevaluatePipeline\u001b[1;34m(pipeline, pipelineName, param_grid, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy:    \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precission:  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy train:    \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precission train:  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Recall:  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\notebooks\\env_1\\env\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\notebooks\\env_1\\env\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and binary targets"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\t'Classifier__estimator__alpha': [0.0001,  0.001,  0.005,  0.01],\n",
    "    'Classifier__estimator__penalty': [\"l2\", \"l1\", \"elasticnet\"],    \n",
    "    'Classifier__estimator__loss': [\"hinge\", \"log\", \"squared_loss\", \"huber\", \"perceptron\"]\n",
    "}\n",
    "\n",
    "pipeline = create_classification_pipeline(SGDClassifier(max_iter=2000, early_stopping=True))\n",
    "evaluatePipeline(pipeline, \"SGDClassifier\", param_grid, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('pca',\n",
      "                 PCA(copy=True, iterated_power=10, n_components=None,\n",
      "                     random_state=None, svd_solver='randomized', tol=0.0,\n",
      "                     whiten=False)),\n",
      "                ('standardscaler',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=1.0,\n",
      "                               max_delta_step=0, max_depth=2,\n",
      "                               min_child_weight=14, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=1,\n",
      "                               objective='binary:logistic', random_state=0,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=0.8,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "env_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
